url: "http://ruta.software"

template:
  params:
    bootswatch: lumen

authors:
  David Charte:
    href: http://david.quotient.space

development:
  mode: unreleased

navbar:
  title: Ruta
  type: default
  left:
    - text: "Home"
      href: ""
      icon: fa-home
    - text: "Get started"
      href: "articles/getting-started.html"
      icon: fa-code
  right:
    - text: "Documentation"
      icon: fa-book
      href: "reference/"
    - icon: fa-github
      href: "https://github.com/fdavidcl/ruta"

reference:
  - title: "Neural network architecture"
    desc: >
      This set of functions provide the necessary functionality to define the
      neural architectures of autoencoders, by connecting layers of units.
    contents:
      - input
      - dense
      - output
      - "c.ruta_network"
      - "[.ruta_network"
      - "print.ruta_network"
      - "plot.ruta_network"
      - new_layer
      - new_network
      - as_network
      - encoding_index
  - title: "Autoencoder and variants"
    desc: >
      These functions allow to create and customize autoencoder learners.
    contents:
      - autoencoder
      - new_autoencoder
      - add_weight_decay
      - weight_decay
      - autoencoder_sparse
      - make_sparse
      - sparsity
      - is_sparse
      - autoencoder_contractive
      - make_contractive
      - contraction
      - autoencoder_denoising
      - make_denoising
      - autoencoder_robust
      - make_robust
      - correntropy
      - autoencoder_variational
  - title: "Model training"
    desc: >
      The following functions allow to train an autoencoder with input data.
    contents:
      - autoencode
      - starts_with("apply_filter")
      - train.ruta_autoencoder
  - title: "Keras conversions"
    desc: >
      These are internal functions which convert Ruta wrapper objects into
      Keras objects and functions.
    contents:
      - starts_with("to_keras")
  - title: "Tasks for trained models"
    desc: >
      The following functions can be applied when an autoencoder has been
      trained, in order to transform data from the input space onto the latent
      space and viceversa.
    contents:
      - encode
      - decode

) %>%
train(x_train, epochs = 40) %>%
evaluate_mean_squared_error(x_test)
devtools::load_all("/media/datos/Documents/research/software/ruta")
library(purrr)
# Shuffle and normalize dataset
x <- iris[, 1:4] %>% sample %>% as.matrix %>% scale
x_train <- x[1:100, ]
x_test <- x[101:150, ]
autoencoder_sparse(
input() + dense(256) + dense(36) + dense(256) + output("sigmoid"),
loss = "mean_squared_error"
) %>%
train(x_train, epochs = 40) %>%
evaluate_mean_squared_error(x_test)
library(purrr)
# Shuffle and normalize dataset
x <- iris[, 1:4] %>% sample %>% as.matrix %>% scale
x_train <- x[1:100, ]
x_test <- x[101:150, ]
autoencoder_contractive(
input() + dense(256) + dense(36) + dense(256) + output("sigmoid"),
loss = "mean_squared_error"
) %>%
train(x_train, epochs = 40) %>%
evaluate_mean_squared_error(x_test)
library(purrr)
# Shuffle and normalize dataset
x <- iris[, 1:4] %>% sample %>% as.matrix %>% scale
x_train <- x[1:100, ]
x_test <- x[101:150, ]
aaa <- autoencoder_contractive(
input() + dense(256) + dense(36) + dense(256) + output("sigmoid"),
loss = "mean_squared_error"
) %>%
train(x_train, epochs = 40)
decode(aaa, encode(aaa, x_test))
net <- input() +
conv(16, 3, activation = "relu", max_pooling = 2) +
conv( 8, 3, activation = "relu", max_pooling = 2) +
conv( 8, 3, activation = "relu", upsampling = 2) +
conv(16, 3, activation = "relu", upsampling = 2) +
conv(1, 3, activation = "sigmoid")
ae <- autoencoder_denoising(net, loss = "binary_crossentropy")
model <-
ae %>% train(x_train, validation_data = x_test, epochs = 4)
dim(x_train)
x_train
mnist <- dataset_mnist()
#' **This example demonstrates a possible implementation of convolutional autoencoders with the Ruta package.**
#'
#' Convolutional layers are defined with `conv` indicating number of learned filters, size of the kernels and whether there are
#' max/average pooling or upsampling operations to be made.
library(magrittr)
library(keras)
library(ruta)
mnist <- dataset_mnist()
x_train <- mnist$train$x / 255.0
x_test <- mnist$test$x / 255.0
# convert to shape: (batch_size, 28, 28, 1)
x_train <- array_reshape(x_train, c(dim(x_train), 1))
x_test <- array_reshape(x_test, c(dim(x_test), 1))
net <- input() +
conv(16, 3, activation = "relu", max_pooling = 2) +
conv( 8, 3, activation = "relu", max_pooling = 2) +
conv( 8, 3, activation = "relu", upsampling = 2) +
conv(16, 3, activation = "relu", upsampling = 2) +
conv(1, 3, activation = "sigmoid")
ae <- autoencoder_denoising(net, loss = "binary_crossentropy")
model <-
ae %>% train(x_train, validation_data = x_test, epochs = 4)
evaluate_mean_squared_error(model, x_test)
library(cranlogs)
dat <- cran_downloads(from = "2018-05-08", packages = "ruta")
plot(count ~ date, data = dat)
plot(count ~ date, data = dat, pch = 24)
plot(count ~ date, data = dat, pch = 21)
plot(count ~ date, data = dat, pch = 20)
library(purrr)
cumulative <- map_int(1:length(dat$count), ~ sum(dat$count[1:.]))
cumulative <- map_dbl(1:length(dat$count), ~ sum(dat$count[1:.]))
plot(cumulative ~ date, data = dat, pch = 20)
plot(cumulative ~ date, data = dat, type = "l")
devtools::load_all("/media/datos/Documents/research/software/ruta")
pkgdown::build_site()
devtools::load_all("/media/datos/Documents/research/software/ruta")
autoencoder(
input() + dense(256) + dense(36, "tanh") + dense(256) + output("sigmoid"),
loss = "mean_squared_error"
) %>%
make_contractive(weight = 1e-4) %>%
train(x_train, epochs = 40) %>%
evaluate_mean_squared_error(x_test)
x <- iris[, 1:4] %>% sample %>% as.matrix %>% scale
x_train <- x[1:100, ]
x_test <- x[101:150, ]
autoencoder(
input() + dense(256) + dense(36, "tanh") + dense(256) + output("sigmoid"),
loss = "mean_squared_error"
) %>%
make_contractive(weight = 1e-4) %>%
train(x_train, epochs = 40) %>%
evaluate_mean_squared_error(x_test)
devtools::load_all("/media/datos/Documents/research/software/ruta")
x <- iris[, 1:4] %>% sample %>% as.matrix %>% scale
x_train <- x[1:100, ]
x_test <- x[101:150, ]
autoencoder(
input() + dense(256) + dense(36, "tanh") + dense(256) + output("sigmoid"),
loss = "mean_squared_error"
) %>%
make_contractive(weight = 1e-4) %>%
train(x_train, epochs = 40) %>%
evaluate_mean_squared_error(x_test)
devtools::load_all("/media/datos/Documents/research/software/ruta")
x <- iris[, 1:4] %>% sample %>% as.matrix %>% scale
x_train <- x[1:100, ]
x_test <- x[101:150, ]
autoencoder(
input() + dense(256) + dense(36, "tanh") + dense(256) + output("sigmoid"),
loss = "mean_squared_error"
) %>%
make_contractive(weight = 1e-4) %>%
train(x_train, epochs = 40) %>%
evaluate_mean_squared_error(x_test)
devtools::load_all("/media/datos/Documents/research/software/ruta")
x <- iris[, 1:4] %>% sample %>% as.matrix %>% scale
x_train <- x[1:100, ]
x_test <- x[101:150, ]
autoencoder(
input() + dense(256) + dense(36, "tanh") + dense(256) + output("sigmoid"),
loss = "mean_squared_error"
) %>%
make_contractive(weight = 1e-4) %>%
train(x_train, epochs = 40) %>%
evaluate_mean_squared_error(x_test)
test_function()
devtools::load_all("/media/datos/Documents/research/software/ruta")
test_function()
devtools::load_all("/media/datos/Documents/research/software/ruta")
test_function()
devtools::load_all("/media/datos/Documents/research/software/ruta")
test_function()
devtools::load_all("/media/datos/Documents/research/software/ruta")
test_function()
test_function(input())
arg_loss()
arg_loss(default = F)
arg_loss(get = T)
which_args(conv)
which_functions()
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(conv)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(conv)
which_args(autoencoder)
which_args(test_function)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(test_function)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(test_function)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(test_function)
test_function(input(), "cosi")
devtools::load_all("/media/datos/Documents/research/software/ruta")
test_function(input(), "cosi")
which_args(test_function)
which_args(autoencoder)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(autoencoder)
help()
help("help")
which_args(autoencoder)
which_args(which_args)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(which_args)
which_args()
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args()
which_args(which_args)
get_keras_object
mget(ls(pattern = "^loss_\\d+$", pos = "package:keras"))
mget(ls(pattern = "^loss_\\d+$", pos = "keras"))
mget(ls(pattern = "^loss_\\d+$", pos = "pkg:keras"))
ls(pattern = "^loss_\\d+$", envir = asNamespace("keras"))
ls(pattern = "^loss_", envir = asNamespace("keras"))
list_keras_objects <- function(prefix, rm_prefix = TRUE) {
found <- ls(pattern = paste0("^", prefix), envir = asNamespace("keras"))
if (rm_prefix) {
gsub(prefix, "", found, fixed = TRUE)
} else {
found
}
}
list_keras_objects("loss")
devtools::load_all("/media/datos/Documents/research/software/ruta")
list_keras_objects("loss")
View(list_keras_objects)
rm(list_keras_objects())
rm(list_keras_objects)
list_keras_objects()
list_keras_objects("loss")
list_keras_objects("layer")
list_keras_objects("activation")
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(test_function)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(test_function)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(test_function)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(test_function)
devtools::load_all("/media/datos/Documents/research/software/ruta")
which_args(test_function)
source('/media/datos/keep-sync/research/software/ae-models/reductive_experiments.R')
results
resultsnoae <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(FALSE, "knn")
source('/media/datos/keep-sync/research/software/ae-models/reductive_experiments.R')
resultsnoae
results
results <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M", normalize = FALSE) %>%
experiment(autoencoder, "knn")
results <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(autoencoder, "knn", normalize = FALSE)
results
source('/media/datos/keep-sync/research/software/ae-models/reductive_experiments.R')
results
results <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(autoencoder, "knn", normalize = TRUE)
results
resultsnoae
install.packages("keras")
install.packages("keras")
source('/media/datos/keep-sync/research/software/ae-models/reductive_experiments.R')
results
train_model <- function(autoencoder_f, method, train_x, train_y, normalized) {
if (is.logical(autoencoder_f) && autoencoder_f == FALSE) {
feature_extractor <- function(x) return(x)
features <- train_x
} else {
## Extract features
hidden_dim <- ceiling(0.1 * dim(train_x)[2])
activation <- if (normalized) "sigmoid" else "linear"
network <- input() + dense(hidden_dim, "relu") + output(activation)
# Do not use binary crossentropy (and sigmoid activation) *unless* the data has been
# accordingly normalized (to the [0, 1] interval)
loss <- if (normalized) "binary_crossentropy" else "mean_squared_error"
feature_extractor <- autoencoder_f(network, loss = loss)
print(feature_extractor)
feature_extractor <- feature_extractor %>% ruta::train(train_x, epochs = 1000)
feature_extractor <- purrr::compose(name, purrr::partial(ruta::encode, learner = feature_extractor, .lazy = FALSE))
features <- feature_extractor(train_x)
}
str(features)
## Classifier
# ctrl <- trainControl()
classifier <- caret::train(features, train_y, method = method)
print(classifier)
list(
feature_extractor = feature_extractor,
classifier = classifier
)
}
results <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(autoencoder, "knn", normalize = TRUE)
resultsae
results
devtools::load_all("/media/datos/Documentos/research/software/ruta")
resultsaered <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(autoencoder_reductive, "knn", normalize = TRUE)
devtools::load_all("/media/datos/Documentos/research/software/ruta")
resultsaered <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(autoencoder_reductive, "knn", normalize = TRUE)
devtools::load_all("/media/datos/Documentos/research/software/ruta")
resultsaered <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(autoencoder_reductive, "knn", normalize = TRUE)
train_model <- function(autoencoder_f, method, train_x, train_y, normalized) {
if (is.logical(autoencoder_f) && autoencoder_f == FALSE) {
feature_extractor <- function(x) return(x)
features <- train_x
} else {
## Extract features
hidden_dim <- ceiling(0.1 * dim(train_x)[2])
activation <- if (normalized) "sigmoid" else "linear"
network <- input() + dense(hidden_dim, "relu") + output(activation)
# Do not use binary crossentropy (and sigmoid activation) *unless* the data has been
# accordingly normalized (to the [0, 1] interval)
loss <- if (normalized) "binary_crossentropy" else "mean_squared_error"
feature_extractor <- autoencoder_f(network, loss = loss)
print(feature_extractor)
feature_extractor <- if (autoencoder_f == ruta::autoencoder_reductive)
feature_extractor %>% ruta::train(train_x, classes = train_y, epochs = 200)
else
feature_extractor %>% ruta::train(train_x, epochs = 200)
feature_extractor <- purrr::compose(name, purrr::partial(ruta::encode, learner = feature_extractor, .lazy = FALSE))
features <- feature_extractor(train_x)
}
str(features)
## Classifier
# ctrl <- trainControl()
classifier <- caret::train(features, train_y, method = method)
print(classifier)
list(
feature_extractor = feature_extractor,
classifier = classifier
)
}
resultsaered <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(autoencoder_reductive, "knn", normalize = TRUE)
devtools::load_all("/media/datos/Documentos/research/software/ruta")
resultsaered <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(autoencoder_reductive, "knn", normalize = TRUE)
train_model <- function(autoencoder_f, method, train_x, train_y, normalized) {
if (is.logical(autoencoder_f) && autoencoder_f == FALSE) {
feature_extractor <- function(x) return(x)
features <- train_x
} else {
## Extract features
hidden_dim <- ceiling(0.1 * dim(train_x)[2])
activation <- if (normalized) "sigmoid" else "linear"
network <- input() + dense(hidden_dim, "relu") + output(activation)
# Do not use binary crossentropy (and sigmoid activation) *unless* the data has been
# accordingly normalized (to the [0, 1] interval)
loss <- if (normalized) "binary_crossentropy" else "mean_squared_error"
feature_extractor <- autoencoder_f(network, loss = loss)
print(feature_extractor)
feature_extractor <- if (is_reductive(feature_extractor))
feature_extractor %>% ruta::train(train_x, classes = train_y, epochs = 200)
else
feature_extractor %>% ruta::train(train_x, epochs = 200)
feature_extractor <- purrr::compose(name, purrr::partial(ruta::encode, learner = feature_extractor, .lazy = FALSE))
features <- feature_extractor(train_x)
}
str(features)
## Classifier
# ctrl <- trainControl()
classifier <- caret::train(features, train_y, method = method)
print(classifier)
list(
feature_extractor = feature_extractor,
classifier = classifier
)
}
resultsaered <- read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M") %>%
experiment(autoencoder_reductive, "knn", normalize = TRUE)
read.csv("wdbc.data")[, 2:32] %>%
preparation("M", "M")
wdbc <- read.csv("wdbc.data")[, 2:32] %>% preparation("M", "M")
class(wdbc)
class(wdbc$1)
class(wdbc$x)
class(wdbc$y)
as.numeric(wdbc$y)
as.numeric(wdbc$y) -1
train_model <- function(autoencoder_f, method, train_x, train_y, normalized) {
if (is.logical(autoencoder_f) && autoencoder_f == FALSE) {
feature_extractor <- function(x) return(x)
features <- train_x
} else {
## Extract features
hidden_dim <- ceiling(0.1 * dim(train_x)[2])
activation <- if (normalized) "sigmoid" else "linear"
network <- input() + dense(hidden_dim, "relu") + output(activation)
# Do not use binary crossentropy (and sigmoid activation) *unless* the data has been
# accordingly normalized (to the [0, 1] interval)
loss <- if (normalized) "binary_crossentropy" else "mean_squared_error"
feature_extractor <- autoencoder_f(network, loss = loss)
print(feature_extractor)
feature_extractor <- if (is_reductive(feature_extractor))
feature_extractor %>% ruta::train(train_x, classes = as.numeric(train_y) - 1, epochs = 200)
else
feature_extractor %>% ruta::train(train_x, epochs = 200)
feature_extractor <- purrr::compose(name, purrr::partial(ruta::encode, learner = feature_extractor, .lazy = FALSE))
features <- feature_extractor(train_x)
}
str(features)
## Classifier
# ctrl <- trainControl()
classifier <- caret::train(features, train_y, method = method)
print(classifier)
list(
feature_extractor = feature_extractor,
classifier = classifier
)
}
resultsaered <- wdbc %>% experiment(autoencoder_reductive, "knn", normalize = TRUE)
resultsaered
devtools::load_all("/media/datos/Documentos/research/software/ruta")
arg_loss("mean_squared_error")
f <- function(...) {print(list(...))}
f(ruta_loss = NULL)
names(f(ruta_loss = NULL))
list(NULL)
a <- which_args(autoencoder)
a
a$network
devtools::load_all("/media/datos/Documentos/research/software/ruta")
arg_constructor(
ruta_loss = NULL,
character = list_keras_objects("loss"),
.default = "mean_squared_error"
)
arg_constructor(
ruta_loss = NULL,
character = list_keras_objects("loss"),
.default = "mean_squared_error"
)(get = T)
arg_constructor(
ruta_loss = NULL,
character = list_keras_objects("loss"),
.default = "mean_squared_error"
)()
arg_constructor(
ruta_loss = NULL,
character = list_keras_objects("loss"),
.default = "mean_squared_error"
)("cosi")
a
f
f()
is.null(f())
length(f())
devtools::load_all("/media/datos/Documentos/research/software/ruta")
is.atomic(NULL)
function() { invisible(1); return(3) }
(function() { invisible(1); return(3) })()
(function() { return(invisible(1)); return(3) })()
(function() { return(invisible(1)); return(3) })() == 1
3$A
paste0(list("a", "b"))
paste0(list("a", "b"), collapse = " ")
append(list(), a)
append(list(), 4)
append(list(2), a)
append(list(2), 56)
paste0("[", NULL, ", ", 60, "]")
is.null(c(3, 5, 6))
c(NULL, 5, 6)
NULL || 3
NULL | 3
NULL %||% 3
NULL %||% TRUE
NULL %||% FALSE
devtools::load_all("/media/datos/Documentos/research/software/ruta")
devtools::load_all("/media/datos/Documentos/research/software/ruta")
devtools::load_all("/media/datos/Documentos/research/software/ruta")
devtools::load_all("/media/datos/Documentos/research/software/ruta")
which_args(autoencoder)
for (x in list(autoencoder)) { which_args(x)}
for (x in list(autoencoder)) { print(which_args(x))}
for (x in list("autoencoder")) { print(which_args(x)) }
purrr::map(list(input, output), which_args)
array(c(1, 2, 3, 4, 5, 6, 7, 8, 9))
array(c(1, 2, 3, 4, 5, 6, 7, 8, 9), dim = c(3, 3))
m <- array(c(1, 2, 3, 4, 5, 6, 7, 8, 9), dim = c(3, 3))
devtools::load_all("/media/datos/Documentos/research/software/ruta")
noise("zeros") %>% apply_filter(m)
noise("zeros") %>% apply_filter(m)
noise("ones") %>% apply_filter(m)
noise("ones") %>% apply_filter(m)
noise("ones") %>% apply_filter(m)
noise("ones") %>% apply_filter(m)
noise("ones") %>% apply_filter(m)
noise("ones", p = 0.5) %>% apply_filter(m)
noise("ones", p = 0.5) %>% apply_filter(m)
noise("saltpepper", p = 0.5) %>% apply_filter(m)
noise("gaussian", sd = 0.5) %>% apply_filter(m)
noise("gaussian", sd = 0.5) %>% apply_filter(m)
noise("cauchy") %>% apply_filter(m)
noise("cauchy") %>% apply_filter(m)
noise("cauchy") %>% apply_filter(m)
m <- array(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), dim = c(2, 2, 3))
noise("cauchy") %>% apply_filter(m)
m
network <-
input() +
dense(256, "elu") +
variational_block(4, seed = 42, epsilon_std = .5) +
dense(256, "elu") +
output("sigmoid")
learner <- autoencoder_variational(network, loss = "binary_crossentropy")
devtools::load_all("/media/datos/Documents/research/software/ruta")
network <-
input() +
dense(256, "elu") +
variational_block(4, seed = 42, epsilon_std = .5) +
dense(256, "elu") +
output("sigmoid")
learner <- autoencoder_variational(network, loss = "binary_crossentropy")
x_train_mat <- matrix(1:200, nrow = 10)
model <- learner %>% train(x_train_mat, epochs = 5)
model$models
inputs <- get_layer(model$models[[1]], index = 1)$input
inputs <- keras::get_layer(model$models[[1]], index = 1)$input
latent_space <- keras::get_layer(model$models[[1]], index = 7)$output
latent_model <- keras::keras_model(
inputs = inputs,
outputs = latent_space
)
latent_model
